{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c513198",
   "metadata": {},
   "source": [
    "# Linguistic Annotation -- ZH (word) -- surprisal, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c94365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Current working directory: /swdata/yin/Cui/Re-Veil/minicons\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"üìÅ Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa92a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wordfreq import tokenize\n",
    "from wordfreq import zipf_frequency\n",
    "from wordfreq import word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb89396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Reading file from: /swdata/yin/Cui/Re-Veil/Re-Veil/post_processing/data/zh/trials/onestop_zh.tsv\n",
      "‚úÖ File loaded. Rows: 60\n",
      "üßπ Dropped unused columns.\n",
      "üìö Cleaned and split text into paragraphs. Total: 65\n"
     ]
    }
   ],
   "source": [
    "file_path = '/swdata/yin/Cui/Re-Veil/Re-Veil/post_processing/data/zh/trials/onestop_zh.tsv'\n",
    "\n",
    "print(f\"üìÑ Reading file from: {file_path}\")\n",
    "df = pd.read_csv(file_path, sep='\\t', dtype={\n",
    "    \"experiment\": str, \"experiment_id\": str, \"condition_id\": str,\n",
    "    \"stimulus_id\": str, \"stimulus_name\": str, \"page_id\": str,\n",
    "    \"page_name\": str, \"item_id\": str, \"question_id\": str, \"response_true\": str\n",
    "})\n",
    "print(f\"‚úÖ File loaded. Rows: {len(df)}\")\n",
    "\n",
    "# Drop unused columns\n",
    "df.drop(columns=[\n",
    "    'question', 'distractor_1', 'distractor_2', 'distractor_3'\n",
    "], inplace=True)\n",
    "print(\"üßπ Dropped unused columns.\")\n",
    "\n",
    "# Preprocessing\n",
    "df['experiment_id'] = 'zh1'\n",
    "df = df.assign(text=df['text'].str.split(\"@#@\")).explode('text')\n",
    "\n",
    "df['para_id'] = df.groupby(\"item_id\").cumcount()\n",
    "\n",
    "# Remove empty/whitespace-only paragraphs\n",
    "df = df[df['text'].str.strip().astype(bool)]\n",
    "print(f\"üìö Cleaned and split text into paragraphs. Total: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5aa2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "# tokenize the text\n",
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "df['tokens'] = df['text'].apply(tokenize_text)\n",
    "# Explode the tokens\n",
    "df = df.explode('tokens')\n",
    "df.drop(columns=['text'], inplace=True)\n",
    "df['page_token_id'] = df.groupby('item_id').cumcount()\n",
    "df['para_token_id'] = df.groupby([\"item_id\", \"para_id\"]).cumcount()\n",
    "df['stim_token_id'] = df.groupby(\"stimulus_id\").cumcount()\n",
    "df['exp_token_id'] = df.groupby(\"experiment_id\").cumcount()\n",
    "\n",
    "# Create a new column 'char' with the splited characters in tokens\n",
    "df['word'] = df['tokens'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b6f575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‰∏Ä\n"
     ]
    }
   ],
   "source": [
    "df = df.explode('word')\n",
    "print(df['word'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36dd5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['page_word_id'] = df.groupby('item_id').cumcount()\n",
    "df['para_word_id'] = df.groupby([\"item_id\", \"para_id\"]).cumcount()\n",
    "df['stim_word_id'] = df.groupby(\"stimulus_id\").cumcount()\n",
    "df['exp_word_id'] = df.groupby(\"experiment_id\").cumcount()\n",
    "\n",
    "df['word_nr'] = df.apply(lambda row: str(row['para_id']) + '-' + str(row['para_word_id']),\n",
    "                                                  axis=1)\n",
    "df['word_nr'] = df['word_nr'].astype(str)\n",
    "df = df.rename(columns={'experiment_id': 'expr_id', 'condition_id': 'cond_id', 'stimulus_id': 'stim_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e3e280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  experiment expr_id condition cond_id stim_id   stimulus_name page_id  \\\n",
      "0  reveil-zh     zh1      real       1       1  Bottle-Message       1   \n",
      "0  reveil-zh     zh1      real       1       1  Bottle-Message       1   \n",
      "0  reveil-zh     zh1      real       1       1  Bottle-Message       1   \n",
      "0  reveil-zh     zh1      real       1       1  Bottle-Message       1   \n",
      "0  reveil-zh     zh1      real       1       1  Bottle-Message       1   \n",
      "\n",
      "  item_id question_id response_true  ...  page_token_id para_token_id  \\\n",
      "0       1         NaN           NaN  ...              0             0   \n",
      "0       1         NaN           NaN  ...              0             0   \n",
      "0       1         NaN           NaN  ...              0             0   \n",
      "0       1         NaN           NaN  ...              0             0   \n",
      "0       1         NaN           NaN  ...              1             1   \n",
      "\n",
      "   stim_token_id  exp_token_id  word  page_word_id para_word_id  stim_word_id  \\\n",
      "0              0             0     ‰∏Ä             0            0             0   \n",
      "0              0             0     Áôæ             1            1             1   \n",
      "0              0             0     Èõ∂             2            2             2   \n",
      "0              0             0     ‰∏Ä             3            3             3   \n",
      "0              1             1     Âπ¥             4            4             4   \n",
      "\n",
      "   exp_word_id  word_nr  \n",
      "0            0      0-0  \n",
      "0            1      0-1  \n",
      "0            2      0-2  \n",
      "0            3      0-3  \n",
      "0            4      0-4  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6c9a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word in column 'word', get the frequency\n",
    "# if the word is UNK], set the frequency to the frequency of the Chinese quatotion mark ‚Äú\n",
    "def get_zipf_frequency(word):\n",
    "    if word == '[UNK]':\n",
    "        return zipf_frequency('‚Äú', 'zh')\n",
    "    else:\n",
    "        return zipf_frequency(word, 'zh')\n",
    "\n",
    "def get_normal_frequency(word):\n",
    "    if word == '[UNK]':\n",
    "        return word_frequency('‚Äú', 'zh')\n",
    "    else:\n",
    "        return word_frequency(word, 'zh')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b37ef5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /swdata/yin/miniconda3/envs/annotation/lib/python3.10/site-packages/wordfreq/data/jieba_zh.txt ...\n",
      "Loading model from cache /tmp/jieba.u009235f70d051c3f8b73eccee1f6adf1.cache\n",
      "Loading model cost 0.059 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Frequency features added.\n",
      "üîç Token length feature added.\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the 'word' column\n",
    "df['zipf_freq'] = df['tokens'].apply(get_zipf_frequency)\n",
    "df['normal_freq'] = df['tokens'].apply(get_normal_frequency)\n",
    "print(\"‚úÖ Frequency features added.\")\n",
    "\n",
    "# Add token length\n",
    "df['token_len'] = df['tokens'].apply(lambda x: len(x) if x != '[UNK]' else int(1))\n",
    "print(\"üîç Token length feature added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f759d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_chr = pd.read_csv('/swdata/yin/Cui/Re-Veil/Re-Veil/post_processing/data/zh/trials/onestop_zh_annotation_chr.csv', dtype={\n",
    "    \"experiment\": str, \"expr_id\": str, \"cond_id\": str, \"condition\": str,\n",
    "    \"stim_id\": str, \"stimulus_name\": str, \"page_id\": str,\n",
    "     \"item_id\": str, \"question_id\": str, \"response_true\": str\n",
    "})\n",
    "# drop the columns that are not needed: zipf_freq, normal_freq, word_len\n",
    "annotation_chr.drop(columns=['zipf_freq', 'normal_freq', 'word_len'], inplace=True)\n",
    "shared_columns = [\n",
    "    'experiment', 'expr_id', 'condition', 'cond_id', 'stim_id', 'stimulus_name',\n",
    "    'page_id', 'item_id', 'question_id', 'response_true',\n",
    "    'para_id', 'page_word_id', 'para_word_id', 'stim_word_id', 'exp_word_id', 'word_nr'\n",
    "]\n",
    "\n",
    "df = df.merge(annotation_chr, on=shared_columns, how='left')\n",
    "df = df.drop(columns=['word_y'])\n",
    "df.rename(columns={'word_x': 'word'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "391f239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = df.groupby('exp_token_id')[['surp', 'entropy', 'renyi', 'minsurp']].sum().reset_index()\n",
    "df = df.drop(columns=['surp', 'entropy', 'renyi', 'minsurp'])  # Remove old values\n",
    "df = df.merge(agg_df, on='exp_token_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c129bedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved to: /swdata/yin/Cui/Re-Veil/Re-Veil/post_processing/data/zh/trials/onestop_zh_annotation_words.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = '/swdata/yin/Cui/Re-Veil/Re-Veil/post_processing/data/zh/trials/onestop_zh_annotation_words.csv'\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"üíæ Saved to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annotation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
