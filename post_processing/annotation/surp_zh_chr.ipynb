{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c513198",
   "metadata": {},
   "source": [
    "# Linguistic Annotation -- ZH (Chr) -- surprisal, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c94365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Current working directory: /swdata/yin/Cui/Re-Veil/minicons\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"üìÅ Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa92a09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/swdata/yin/miniconda3/envs/transocr-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from minicons import scorer\n",
    "from wordfreq import tokenize\n",
    "from wordfreq import zipf_frequency\n",
    "from wordfreq import word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b0114a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading model and tokenizer...\n",
      "‚úÖ Model and tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "# Set the device\n",
    "# device = 'cuda:0'\n",
    "\n",
    "# Load only if not already loaded in the notebook\n",
    "if 'scorer_model' not in globals():\n",
    "    print(\"üîß Loading model and tokenizer...\")\n",
    "    MODEL = \"uer/gpt2-xlarge-chinese-cluecorpussmall\"\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(MODEL, return_dict=True)\n",
    "    model = GPT2LMHeadModel.from_pretrained(MODEL)\n",
    "    scorer_model = scorer.IncrementalLMScorer(model, tokenizer=tokenizer, device='cpu', stride=200)\n",
    "    print(\"‚úÖ Model and tokenizer loaded.\")\n",
    "else:\n",
    "    print(\"‚úÖ Reusing existing model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64fca9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Reading file from: /swdata/yin/Cui/Re-Veil/Re-Veil/post_processing/data/zh/trials/onestop_zh.tsv\n",
      "‚úÖ File loaded. Rows: 60\n",
      "üßπ Dropped unused columns.\n"
     ]
    }
   ],
   "source": [
    "file_path = '/swdata/yin/Cui/Re-Veil/Re-Veil/post_processing/data/zh/trials/onestop_zh.tsv'\n",
    "\n",
    "print(f\"üìÑ Reading file from: {file_path}\")\n",
    "df = pd.read_csv(file_path, sep='\\t', dtype={\n",
    "    \"experiment\": str, \"experiment_id\": str, \"condition_id\": str,\n",
    "    \"stimulus_id\": str, \"stimulus_name\": str, \"page_id\": str,\n",
    "    \"page_name\": str, \"item_id\": str, \"question_id\": str, \"response_true\": str\n",
    "})\n",
    "print(f\"‚úÖ File loaded. Rows: {len(df)}\")\n",
    "\n",
    "# Drop unused columns\n",
    "df.drop(columns=[\n",
    "    'question', 'distractor_1', 'distractor_2', 'distractor_3'\n",
    "], inplace=True)\n",
    "print(\"üßπ Dropped unused columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b327fc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Cleaned and split text into paragraphs. Total: 65\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "df['experiment_id'] = 'zh1'\n",
    "df = df.assign(text=df['text'].str.split(\"@#@\")).explode('text')\n",
    "\n",
    "df['para_id'] = df.groupby(\"item_id\").cumcount()\n",
    "\n",
    "# Remove empty/whitespace-only paragraphs\n",
    "df = df[df['text'].str.strip().astype(bool)]\n",
    "print(f\"üìö Cleaned and split text into paragraphs. Total: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d289bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Calculating token-level surprisal scores (this may take a while)...\n",
      "‚úÖ Surprisal scores computed.\n",
      "0     [[([CLS], 0.0, 0, 0.0, 0.0, 0.0), (‰∏Ä, 6.035219...\n",
      "0     [[([CLS], 0.0, 0, 0.0, 0.0, 0.0), (Êù®, 10.61592...\n",
      "1     [[([CLS], 0.0, 0, 0.0, 0.0, 0.0), (Ëøô, 5.078294...\n",
      "2     [[([CLS], 0.0, 0, 0.0, 0.0, 0.0), ([UNK], 8.49...\n",
      "3     [[([CLS], 0.0, 0, 0.0, 0.0, 0.0), (Èöè, 11.19388...\n",
      "                            ...                        \n",
      "55    [[([CLS], 0.0, 0, 0.0, 0.0, 0.0), (Ë∞à, 12.49851...\n",
      "56    [[([CLS], 0.0, 0, 0.0, 0.0, 0.0), (Ëøô, 5.078294...\n",
      "57    [[([CLS], 0.0, 0, 0.0, 0.0, 0.0), (ËÆ©, 10.37613...\n",
      "58    [[([CLS], 0.0, 0, 0.0, 0.0, 0.0), (ÂáØ, 12.34901...\n",
      "59    [[([CLS], 0.0, 0, 0.0, 0.0, 0.0), ([UNK], 8.49...\n",
      "Name: surp, Length: 65, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"‚öôÔ∏è Calculating token-level surprisal scores (this may take a while)...\")\n",
    "df['surp'] = df['text'].map(lambda x: scorer_model.token_score(x, surprisal=True, base_two=True, rank=True))\n",
    "print(\"‚úÖ Surprisal scores computed.\")\n",
    "\n",
    "print(df['surp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b32873fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before first explode: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Before first explode:\", type(df['surp'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5436a1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first explode: <class 'tuple'>\n",
      "üö´ Filtered special tokens. Remaining tokens: 5238\n"
     ]
    }
   ],
   "source": [
    "# Explode token list\n",
    "\n",
    "\n",
    "df = df.explode('surp')\n",
    "df = df.explode('surp')\n",
    "\n",
    "print(\"After first explode:\", type(df['surp'].iloc[0]))\n",
    "# Split tuple into separate columns\n",
    "df[['word', 'surp', 'rank', 'entropy', 'renyi', 'minsurp']] = pd.DataFrame(df['surp'].tolist(), index=df.index)\n",
    "\n",
    "# Remove [CLS] and [SEP]\n",
    "df = df[~df['word'].isin(['[CLS]', '[SEP]'])]\n",
    "print(f\"üö´ Filtered special tokens. Remaining tokens: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c500b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop temporary columns\n",
    "df.drop(columns=['text'], inplace=True)\n",
    "df['word'] = df['word'].apply(lambda x: list(x) if x != '[UNK]' else x)\n",
    "df = df.explode('word')\n",
    "\n",
    "df['page_word_id'] = df.groupby('item_id').cumcount()\n",
    "df['para_word_id'] = df.groupby([\"item_id\", \"para_id\"]).cumcount()\n",
    "df['stim_word_id'] = df.groupby(\"stimulus_id\").cumcount()\n",
    "df['exp_word_id'] = df.groupby(\"experiment_id\").cumcount()\n",
    "\n",
    "df['word_nr'] = df.apply(lambda row: str(row['para_id']) + '-' + str(row['para_word_id']),\n",
    "                                                  axis=1)\n",
    "df['word_nr'] = df['word_nr'].astype(str)\n",
    "df = df.rename(columns={'experiment_id': 'expr_id', 'condition_id': 'cond_id', 'stimulus_id': 'stim_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6c9a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word in column 'word', get the frequency\n",
    "# if the word is UNK], set the frequency to the frequency of the Chinese quatotion mark ‚Äú\n",
    "def get_zipf_frequency(word):\n",
    "    if word == '[UNK]':\n",
    "        return zipf_frequency('‚Äú', 'zh')\n",
    "    else:\n",
    "        return zipf_frequency(word, 'zh')\n",
    "\n",
    "def get_normal_frequency(word):\n",
    "    if word == '[UNK]':\n",
    "        return word_frequency('‚Äú', 'zh')\n",
    "    else:\n",
    "        return word_frequency(word, 'zh')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b37ef5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Frequency features added.\n",
      "üîç Token length feature added.\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the 'word' column\n",
    "df['zipf_freq'] = df['word'].apply(get_zipf_frequency)\n",
    "df['normal_freq'] = df['word'].apply(get_normal_frequency)\n",
    "print(\"‚úÖ Frequency features added.\")\n",
    "\n",
    "# Add token length\n",
    "df['word_len'] = df['word'].apply(lambda x: len(x) if x != '[UNK]' else int(1))\n",
    "print(\"üîç Token length feature added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c129bedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved to: /swdata/yin/Cui/Re-Veil/Re-Veil/post_processing/data/zh/trials/onestop_zh_annotation_chr.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = '/swdata/yin/Cui/Re-Veil/Re-Veil/post_processing/data/zh/trials/onestop_zh_annotation_chr.csv'\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"üíæ Saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0db925",
   "metadata": {},
   "source": [
    "### TEST FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d048fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import word_frequency\n",
    "sentences = [\"Âú®Â∫ä‰∏äËµñ‰∫Ü‰∏ÄÂ§©ÔºåÊàëÊÉ≥Ëµ∑Â∫äË¥µ\", \"Âú®Â∫ä‰∏äËµñ‰∫Ü‰∏ÄÂ§©ÔºåÊàëÊÉ≥Ëµ∑Â∫ä‰∫Ü\" ]\n",
    "# print(\"üí¨ Example sentence:\", sentences)\n",
    "\n",
    "# print(\"‚öôÔ∏è Calculating token-level surprisal scores for example sentence...\")\n",
    "# surps = scorer_model.token_score(sentences, surprisal=True, base_two=True)\n",
    "\n",
    "# print(\"‚úÖ Surprisal scores computed for example sentence.\")\n",
    "# for ele in surps:\n",
    "#     print(ele)\n",
    "#     # for word, surp, entropy, renyi, minsurp in ele:\n",
    "#     #     print(f\"Word: {word}, Surprisal: {surp}, Entropy: {entropy}, R√©nyi: {renyi}, MinSurp: {minsurp}\")\n",
    "# print(\"üîö Finished processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09378878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23e-05\n"
     ]
    }
   ],
   "source": [
    "print(word_frequency('ÊÇ®‰ª¨', 'zh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f206c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Âú®\n",
      "7.16\n",
      "Â∫ä‰∏ä\n",
      "4.6\n",
      "Ëµñ\n",
      "4.17\n",
      "‰∫Ü\n",
      "7.14\n",
      "‰∏ÄÂ§©\n",
      "5.25\n",
      "Êàë\n",
      "6.95\n",
      "ÊÉ≥\n",
      "6.03\n",
      "Ëµ∑Â∫ä\n",
      "4.23\n",
      "Ë¥µ\n",
      "4.64\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(sentences[0], 'zh')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "    print(zipf_frequency(token, 'zh'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transocr-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
