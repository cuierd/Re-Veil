{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c513198",
   "metadata": {},
   "source": [
    "# Linguistic Annotation -- EN -- surprisal, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c94365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Current working directory: /swdata/yin/Cui/Re-Veil/minicons\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"üìÅ Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa92a09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/swdata/yin/miniconda3/envs/transocr-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from minicons import scorer\n",
    "from wordfreq import tokenize\n",
    "from wordfreq import zipf_frequency\n",
    "from wordfreq import word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b0114a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading model and tokenizer...\n",
      "‚úÖ Model and tokenizer loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/swdata/yin/Cui/Re-Veil/minicons/minicons/scorer.py:1410: UserWarning: tokenizer is changed by adding pad_token_id to the tokenizer.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Set the device\n",
    "# device = 'cuda:0'\n",
    "\n",
    "# Load only if not already loaded in the notebook\n",
    "if 'scorer_model' not in globals():\n",
    "    print(\"üîß Loading model and tokenizer...\")\n",
    "    MODEL = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL, return_dict=True)\n",
    "    scorer_model = scorer.IncrementalLMScorer(model, tokenizer=tokenizer, device='cpu', stride=200, bos_token=True)\n",
    "    print(\"‚úÖ Model and tokenizer loaded.\")\n",
    "else:\n",
    "    print(\"‚úÖ Reusing existing model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c18688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e35c4e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [\"Thought to be the world‚Äôs oldest message in a bottle\"]\n",
    "sentences = [\"The sketch of those trucks hasn't\", \"The sketch of those trucks haven't\"]\n",
    "\n",
    "scores = scorer_model.word_score_tokenized(\n",
    "    batch=sentences,\n",
    "    tokenize_function=whitespace_tokenizer,\n",
    "    bos_token=False,         \n",
    "    eos_token=False,         \n",
    "    surprisal=True,          \n",
    "    base_two=True,            \n",
    "    bow_correction = True,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff5ce3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0),\n",
       "  ('sketch',\n",
       "   15.969964027404785,\n",
       "   12.72680950164795,\n",
       "   14.308034896850586,\n",
       "   6.200345993041992,\n",
       "   15.969964027404785,\n",
       "   12.72680950164795,\n",
       "   14.308034896850586,\n",
       "   6.200345993041992),\n",
       "  ('of',\n",
       "   3.3498291969299316,\n",
       "   6.965978622436523,\n",
       "   11.075484275817871,\n",
       "   2.9639241695404053,\n",
       "   3.3498291969299316,\n",
       "   6.965978622436523,\n",
       "   11.075484275817871,\n",
       "   2.9639241695404053),\n",
       "  ('those',\n",
       "   9.617044448852539,\n",
       "   7.5691118240356445,\n",
       "   12.541632652282715,\n",
       "   1.8073477745056152,\n",
       "   9.617044448852539,\n",
       "   7.5691118240356445,\n",
       "   12.541632652282715,\n",
       "   1.8073477745056152),\n",
       "  ('trucks',\n",
       "   13.100157737731934,\n",
       "   10.958047866821289,\n",
       "   13.142509460449219,\n",
       "   3.6612987518310547,\n",
       "   13.100157737731934,\n",
       "   10.958047866821289,\n",
       "   13.142509460449219,\n",
       "   3.6612987518310547),\n",
       "  (\"hasn't\",\n",
       "   12.35189151763916,\n",
       "   7.586394309997559,\n",
       "   12.198198318481445,\n",
       "   3.5115060806274414,\n",
       "   6.17594575881958,\n",
       "   3.7931971549987793,\n",
       "   6.099099159240723,\n",
       "   1.7557530403137207)],\n",
       " [('The', 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0),\n",
       "  ('sketch',\n",
       "   15.96996784210205,\n",
       "   12.726802825927734,\n",
       "   14.308032989501953,\n",
       "   6.200345993041992,\n",
       "   15.96996784210205,\n",
       "   12.726802825927734,\n",
       "   14.308032989501953,\n",
       "   6.200345993041992),\n",
       "  ('of',\n",
       "   3.3498270511627197,\n",
       "   6.965966701507568,\n",
       "   11.075481414794922,\n",
       "   2.963935136795044,\n",
       "   3.3498270511627197,\n",
       "   6.965966701507568,\n",
       "   11.075481414794922,\n",
       "   2.963935136795044),\n",
       "  ('those',\n",
       "   9.617035865783691,\n",
       "   7.569105625152588,\n",
       "   12.541632652282715,\n",
       "   1.8073477745056152,\n",
       "   9.617035865783691,\n",
       "   7.569105625152588,\n",
       "   12.541632652282715,\n",
       "   1.8073477745056152),\n",
       "  ('trucks',\n",
       "   13.100151062011719,\n",
       "   10.958053588867188,\n",
       "   13.142511367797852,\n",
       "   3.661287784576416,\n",
       "   13.100151062011719,\n",
       "   10.958053588867188,\n",
       "   13.142511367797852,\n",
       "   3.661287784576416),\n",
       "  (\"haven't\",\n",
       "   15.220953941345215,\n",
       "   7.599313259124756,\n",
       "   13.021374702453613,\n",
       "   3.5126616954803467,\n",
       "   7.610476970672607,\n",
       "   3.799656629562378,\n",
       "   6.510687351226807,\n",
       "   1.7563308477401733)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39cf5cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [\"Thought to be the world‚Äôs oldest message in a bottle\", \"The sketch of those trucks haven't\"]\n",
    "\n",
    "# ts = scorer_model.token_score(sentences, surprisal = True, base_two = True)\n",
    "# print(\"Token scores:\")\n",
    "# for i, sentence in enumerate(sentences):\n",
    "#     print(f\"Sentence {i+1}:\")\n",
    "#     for token, surp, entropies, renyi_entropies, min_surprisals in ts[i]:\n",
    "#         print(f\"  Token: {token}, Surprisal: {surp:.4f}, Entropies: {entropies}, Renyi Entropies: {renyi_entropies}, Min Surprisals: {min_surprisals}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64fca9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Reading file from: /swdata/yin/Cui/Re-Veil/Re-Veil/post_processing/data/en/trials/onestop_en.tsv\n",
      "‚úÖ File loaded. Rows: 46\n",
      "üßπ Dropped unused columns.\n"
     ]
    }
   ],
   "source": [
    "file_path = '/swdata/yin/Cui/Re-Veil/Re-Veil/post_processing/data/en/trials/onestop_en.tsv'\n",
    "\n",
    "print(f\"üìÑ Reading file from: {file_path}\")\n",
    "df = pd.read_csv(file_path, sep='\\t', dtype={\n",
    "    \"experiment\": str, \"experiment_id\": str, \"condition_id\": str,\n",
    "    \"stimulus_id\": str, \"stimulus_name\": str, \"page_id\": str,\n",
    "    \"page_name\": str, \"item_id\": str, \"question_id\": str, \"response_true\": str\n",
    "})\n",
    "print(f\"‚úÖ File loaded. Rows: {len(df)}\")\n",
    "\n",
    "# Drop unused columns\n",
    "df.drop(columns=[\n",
    "    'question', 'distractor_1', 'distractor_2', 'distractor_3'\n",
    "], inplace=True)\n",
    "print(\"üßπ Dropped unused columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b327fc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Cleaned and split text into paragraphs. Total: 49\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "df['experiment_id'] = 'en1'\n",
    "df = df.assign(text=df['text'].str.split(\"@#@\")).explode('text')\n",
    "\n",
    "df['para_id'] = df.groupby(\"item_id\").cumcount()\n",
    "\n",
    "# Remove empty/whitespace-only paragraphs\n",
    "df = df[df['text'].str.strip().astype(bool)]\n",
    "print(f\"üìö Cleaned and split text into paragraphs. Total: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d289bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Calculating token-level surprisal scores (this may take a while)...\n",
      "‚úÖ Surprisal scores computed.\n"
     ]
    }
   ],
   "source": [
    "print(\"‚öôÔ∏è Calculating token-level surprisal scores (this may take a while)...\")\n",
    "df['surp'] = df['text'].map(lambda x: scorer_model.word_score_tokenized(\n",
    "    batch=x,\n",
    "    tokenize_function=whitespace_tokenizer,\n",
    "    bos_token=False,         \n",
    "    eos_token=False,         \n",
    "    surprisal=True,          \n",
    "    base_two=True,            \n",
    "    bow_correction = True,  \n",
    "))\n",
    "print(\"‚úÖ Surprisal scores computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b32873fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before first explode: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Before first explode:\", type(df['surp'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5436a1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first explode: <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# Explode token list\n",
    "\n",
    "\n",
    "df = df.explode('surp')\n",
    "df = df.explode('surp')\n",
    "\n",
    "print(\"After first explode:\", type(df['surp'].iloc[0]))\n",
    "# Split tuple into separate columns\n",
    "df[['word', 'surp', 'entropy', 'renyi', 'minsurp', 'msurp', 'mentropy', 'mrenyi', 'mminsurp']] = pd.DataFrame(df['surp'].tolist(), index=df.index)\n",
    "\n",
    "\n",
    "# Remove [CLS] and [SEP]\n",
    "# df = df[~df['word'].isin(['[CLS]', '[SEP]'])]\n",
    "# print(f\"üö´ Filtered special tokens. Remaining tokens: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c500b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop temporary columns\n",
    "df.drop(columns=['text'], inplace=True)\n",
    "# df['word'] = df['word'].apply(lambda x: list(x) if x != '[UNK]' else x)\n",
    "# df = df.explode('word')\n",
    "\n",
    "df['page_word_id'] = df.groupby('item_id').cumcount()\n",
    "df['para_word_id'] = df.groupby([\"item_id\", \"para_id\"]).cumcount()\n",
    "df['stim_word_id'] = df.groupby(\"stimulus_id\").cumcount()\n",
    "df['exp_word_id'] = df.groupby(\"experiment_id\").cumcount()\n",
    "\n",
    "df['word_nr'] = df.apply(lambda row: str(row['para_id']) + '-' + str(row['para_word_id']),\n",
    "                                                  axis=1)\n",
    "df['word_nr'] = df['word_nr'].astype(str)\n",
    "df = df.rename(columns={'experiment_id': 'expr_id', 'condition_id': 'cond_id', 'stimulus_id': 'stim_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6c9a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word in column 'word', get the frequency\n",
    "# if the word is UNK], set the frequency to the frequency of the Chinese quatotion mark ‚Äú\n",
    "def get_zipf_frequency(word):\n",
    "    if word == '[UNK]':\n",
    "        return zipf_frequency('‚Äú', 'en')\n",
    "    else:\n",
    "        return zipf_frequency(word, 'en')\n",
    "\n",
    "def get_normal_frequency(word):\n",
    "    if word == '[UNK]':\n",
    "        return word_frequency('‚Äú', 'en')\n",
    "    else:\n",
    "        return word_frequency(word, 'en')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b37ef5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Frequency features added.\n",
      "üîç Token length feature added.\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the 'word' column\n",
    "df['zipf_freq'] = df['word'].apply(get_zipf_frequency)\n",
    "df['normal_freq'] = df['word'].apply(get_normal_frequency)\n",
    "print(\"‚úÖ Frequency features added.\")\n",
    "\n",
    "# Add token length\n",
    "df['word_len'] = df['word'].apply(lambda x: len(x) if x != '[UNK]' else int(1))\n",
    "print(\"üîç Token length feature added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c129bedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved to: /swdata/yin/Cui/Re-Veil/Re-Veil/post_processing/data/en/trials/onestop_en_annotation_m.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = '/swdata/yin/Cui/Re-Veil/Re-Veil/post_processing/data/en/trials/onestop_en_annotation_m.csv'\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"üíæ Saved to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transocr-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
