{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1706564021871,"user":{"displayName":"Cui Ding","userId":"13036128299667319583"},"user_tz":-60},"id":"GQ9h4Iy9HIDp","outputId":"ff218f41-0eba-42a9-c4cf-cfbb2b225448"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}],"source":["import  torch\n","\n","torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22484,"status":"ok","timestamp":1706563583910,"user":{"displayName":"Cui Ding","userId":"13036128299667319583"},"user_tz":-60},"id":"walG3g8bFiUH","outputId":"f30ff46d-de57-42e6-9b1e-f22bc62380a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNdYjemmWpST"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import matplotlib.backends.backend_pdf as pdf\n","from matplotlib.patches import Circle"]},{"cell_type":"markdown","metadata":{"id":"stKSXcVcx0Fc"},"source":["## Clean raw data by deleting the noise before reading; do not merge samples to associations/associations etc."]},{"cell_type":"code","execution_count":53,"metadata":{"id":"B7mTzkQzs0CI","executionInfo":{"status":"ok","timestamp":1706569104652,"user_tz":-60,"elapsed":28495,"user":{"displayName":"Cui Ding","userId":"13036128299667319583"}}},"outputs":[],"source":["raw = Path(f'/content/drive/MyDrive/MoTR/provo_divided_by_reader').glob('*.csv')\n","out = Path(f'/content/drive/MyDrive/MoTR/provo_plots_test/provo_cleaned_raw_association_not_merged')\n","\n","for f in raw:\n","  df = pd.read_csv(f)\n","\n","  df.loc[:, 'sbm_id'] = df['submission_id'].astype(str)\n","  df.loc[:, 'expr_id'] = df['Experiment'].astype(int)\n","  df.loc[:, 'cond_id'] = df['Condition'].astype(int)\n","  df.loc[:, 'trial_id'] = df['trial_id'].astype(int)\n","  df.loc[:, 'para_nr'] = df['ItemId'].astype(int)\n","  df.loc[:, 'word_nr'] = df['Index'].astype(int)\n","  df.loc[:, 'word'] = df['Word'].astype(str)\n","  df.loc[:, 't'] = df['responseTime'].astype(int)\n","  df.loc[:, 'x'] = df['mousePositionX'].astype(int)\n","  df.loc[:, 'y'] = df['mousePositionY'].astype(int)\n","  df.loc[:, 'wb'] = df['wordPositionBottom'].astype(str)\n","  df.loc[:, 'wt'] = df['wordPositionTop'].astype(str)\n","  df.loc[:, 'wl'] = df['wordPositionLeft'].astype(str)\n","  df.loc[:, 'wr'] = df['wordPositionRight'].astype(str)\n","  df.loc[:, 'response'] = df['response'].astype(str)\n","\n","  dfw = df[['sbm_id', 'expr_id', 'cond_id', 'trial_id', 'para_nr', 'word_nr', 'word', 't',\n","            'x', 'y', 'wb', 'wt', 'wl', 'wr', 'response']]\n","\n","  # # If needed, create a dictionary to map para_nr to trial sequence number\n","  # trial_sequence = {}\n","  # current_trial = 0\n","  # for para_nr in dfw['para_nr'].unique():\n","  #     trial_sequence[para_nr] = current_trial\n","  #     current_trial += 1\n","\n","  # # Add a new column 'trial_nr' to dfw using the trial_sequence mapping\n","  # dfw['trial_id'] = dfw['para_nr'].map(trial_sequence)\n","\n","  grouped_df = dfw.groupby(['cond_id', 'trial_id'])\n","  filtered_df = pd.DataFrame()\n","\n","  for name, group in grouped_df:\n","      filtered_group = group[group['word_nr'].isin([0, 1, 2, 3])]\n","      if not filtered_group.empty:\n","        first_idx = filtered_group.index[0]\n","        # Delete all rows before the first row with 'word_nr' in [0, 1, 2, 3]\n","        group = group.loc[first_idx:]\n","\n","        # Concatenate the filtered group to the filtered_df DataFrame\n","        filtered_df = pd.concat([filtered_df, group], ignore_index=True)\n","\n","\n","  filtered_df = filtered_df.reset_index(drop=True)\n","  filtered_df.to_csv(f'{out}/{f.stem}.csv')"]},{"cell_type":"markdown","metadata":{"id":"EGiiVGuBBGn8"},"source":["## Define a Velocity-based association detection function."]},{"cell_type":"code","execution_count":93,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706572290113,"user":{"displayName":"Cui Ding","userId":"13036128299667319583"},"user_tz":-60},"id":"bNtQmGf_U048"},"outputs":[],"source":["def most_frequent(series):\n","    \"\"\"\n","    Determine the most frequent value in a Pandas Series.\n","    :Parameters series: The Pandas series for which the mode is to be calculated.\n","    \"\"\"\n","    if series.mode().empty:\n","        return \"%2c%\"\n","    mode_value = series.mode()[0]\n","    return mode_value if not pd.isna(mode_value) else \"%2c%\"\n"]},{"cell_type":"code","execution_count":94,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706572293618,"user":{"displayName":"Cui Ding","userId":"13036128299667319583"},"user_tz":-60},"id":"FeKffpD5BTf8"},"outputs":[],"source":["def ivt(gaze_data, vel_thres, dur_thres_fix_low, dur_thres_fix_high, accer_thres, dur_thres_sac):\n","    \"\"\"\n","    Identify associations and saccades in gaze data using the I-VT algorithm.\n","\n","    :param gaze_data: DataFrame with columns ['x', 'y', 't'] for gaze points.\n","    :param vel_thres: Velocity threshold to differentiate saccades from associations.\n","    :param dur_thres_fix_low: Duration threshold to confirm associations (in milliseconds).\n","    :param dur_thres_fix_high: Duration threshold to confirm associations (in milliseconds).\n","    :param accer_thres: Accerelation threshold to detect potential saccades.\n","    :param dur_thres_sac: Duration threshold to confirm saccades (in milliseconds).\n","    :return: DataFrame with an additional 'type' column labeling each point as 'association' or 'saccade'.\n","    \"\"\"\n","    # gaze_data = gaze_data.dropna()\n","\n","    # Calculate distances and velocities\n","    dx = np.diff(gaze_data['x'])\n","    dy = np.diff(gaze_data['y'])\n","    dt = np.diff(gaze_data['t'])\n","\n","    # # Prepend 0 to the differences\n","    dx = np.insert(dx, 0, 0)\n","    dy = np.insert(dy, 0, 0)\n","    dt = np.insert(dt, 0, 0)\n","\n","    # Avoid division by zero\n","    dt = dt.astype(float)\n","    dt[dt == 0] = 1e6\n","\n","    distances = np.sqrt(dx**2 + dy**2)\n","    velocities = distances / dt\n","    dv = np.diff(velocities)\n","    dv = np.insert(dv, 0, 0)\n","    acceleration = dv / dt\n","\n","    # Classify points as associations, saccades or slidings (association_vel + 0.05 px/ms).\n","\n","    gaze_data['type'] = np.where(\n","        (np.absolute(acceleration) < accer_thres) & (velocities < vel_thres),\n","    'association',\n","    np.where(\n","        (np.absolute(acceleration) < accer_thres) & (velocities < vel_thres + 0.05),\n","        'sliding',\n","        'saccade'\n","      )\n","    )\n","    gaze_data['velocities'] = velocities\n","    gaze_data['acceleration'] = acceleration\n","\n","    # Exclude rows where word_nr is -100 (for comprehension question)\n","    gaze_data = gaze_data[gaze_data['word_nr'] != -100]\n","\n","    # Group consecutive points and filter based on duration\n","    gaze_data['group'] = (gaze_data['type'] != gaze_data['type'].shift()).cumsum()\n","    associations = gaze_data[gaze_data['type'] == 'association'].groupby('group').filter(lambda x: dur_thres_fix_low <= (x['t'].iloc[-1] - x['t'].iloc[0]) <= dur_thres_fix_high)\n","    saccades = gaze_data[gaze_data['type'] == 'saccade'].groupby('group').filter(lambda x: (x['t'].iloc[-1] - x['t'].iloc[0]) >= dur_thres_sac)\n","\n","\n","    association_centroid = associations.groupby('group').agg(\n","    sbm_id=('sbm_id', 'first'),\n","    expr_id=('expr_id', 'first'),\n","    cond_id=('cond_id', 'first'),\n","    trial_id = ('trial_id', 'first'),\n","    para_nr=('para_nr', 'first'),\n","    word_nr=('word_nr', most_frequent),\n","    word=('word', most_frequent),\n","    x_mean=('x', 'mean'),\n","    y_mean=('y', 'mean'),\n","    start_t=('t', 'first'),\n","    end_t=('t', 'last')\n","    )\n","    association_centroid['duration'] = association_centroid['end_t'] - association_centroid['start_t']\n","    # Filter out groups where word_nr is -1 or word is null (fix at blank area)\n","    association_centroid = association_centroid[(association_centroid['word_nr'] != -1) & (association_centroid['word'] != \"%2c%\")]\n","\n","    # Calculate statistics for each saccade group\n","    saccade_stats = saccades.groupby('group').agg(\n","        sbm_id=('sbm_id', 'first'),\n","        expr_id=('expr_id', 'first'),\n","        cond_id=('cond_id', 'first'),\n","        trial_id = ('trial_id', 'first'),\n","        para_nr=('para_nr', 'first'),\n","        mean_x=('x', 'mean'),\n","        mean_y=('y', 'mean'),\n","        start_x=('x', 'first'),\n","        end_x=('x', 'last'),\n","        start_y=('y', 'first'),\n","        end_y=('y', 'last'),\n","        start_t=('t', 'first'),\n","        end_t=('t', 'last'),\n","        # mean_velocity=('velocity', 'mean'),\n","        # mean_acceleration=('acceleration', 'mean')\n","    )\n","    saccade_stats['duration'] = saccade_stats['end_t'] - saccade_stats['start_t']\n","\n","    return association_centroid, saccade_stats, gaze_data.iloc[:, 1:-1]"]},{"cell_type":"markdown","metadata":{"id":"yFUKplyZqeNO"},"source":["## Read in all the files and get associations, saccades"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJrfGb9AqUj5"},"outputs":[],"source":["# Here, the setting of thres is different from association\n","vel_thres = 0.1\n","dur_thres_fix_low = 100\n","dur_thres_fix_high = 3000\n","accer_thres = 0.01\n","dur_thres_sac = 60\n","\n","reading_data_path = Path(f'/content/drive/MyDrive/MoTR/provo_cleaned_raw_association_not_merged')\n","\n","# Iterate over each file in the directory\n","for file_path in reading_data_path.iterdir():\n","    # Check if it's a file and not a directory\n","    if file_path.is_file():\n","        print(f\"Processing file: {file_path}\")\n","        reader = str(file_path).split('/')[-1][:-4]\n","        print(f\"Reader: {reader}\")\n","        reading_data = pd.read_csv(file_path)\n","        reading_data.rename(columns={\n","        'submission_id': 'sbm_id',\n","        'Experiment': 'expr_id',\n","        'Condition': 'cond_id',\n","        'ItemId': 'para_nr',\n","        'Index': 'word_nr',\n","        'Word': 'word',\n","        'responseTime': 't',\n","        'mousePositionX': 'x',\n","        'mousePositionY': 'y',\n","        # Uncomment and rename other columns if needed\n","        # 'wordPositionBottom': 'wb',\n","        # 'wordPositionTop': 'wt',\n","        # 'wordPositionLeft': 'wl',\n","        # 'wordPositionRight': 'wr',\n","        'response': 'response'\n","        }, inplace=True)\n","\n","        all_associations = []\n","        all_saccades = []\n","        all_gaze = []\n","\n","        for para_nr in reading_data['para_nr'].unique():\n","            item_data = reading_data[reading_data['para_nr'] == para_nr]\n","            # Extract necessary information\n","            associations, saccades, gaze_data = ivt(item_data, vel_thres, dur_thres_fix_low, dur_thres_fix_high, accer_thres, dur_thres_sac)\n","\n","            # Append associations and saccades to all_associations and all_saccades\n","            all_associations.append(associations)\n","            all_saccades.append(saccades)\n","            all_gaze.append(gaze_data)\n","\n","        # Combine all item results into single DataFrames\n","        all_associations_df = pd.concat(all_associations, ignore_index=True)\n","        all_saccades_df = pd.concat(all_saccades, ignore_index=True)\n","        all_gaze_df = pd.concat(all_gaze, ignore_index=True)\n","\n","        # Write to CSV\n","        all_associations_df.to_csv(f'/content/drive/MyDrive/MoTR/provo_2024/associations_2024/associations_{reader}.csv', index=False)\n","        all_saccades_df.to_csv(f'/content/drive/MyDrive/MoTR/provo_2024/Saccades_2024/Saccades_{reader}.csv', index=False)\n","        all_gaze_df.to_csv(f'/content/drive/MyDrive/MoTR/provo_2024/associations_Saccades_Slidings/associations_Saccades_Slidings_unmerged_{reader}.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"UT8ua8REmGDP"},"source":["## Define a function, take two dfs which has been grouped over cond as arguement."]},{"cell_type":"code","execution_count":96,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706572812890,"user":{"displayName":"Cui Ding","userId":"13036128299667319583"},"user_tz":-60},"id":"_bGJkwhb1l8i"},"outputs":[],"source":["def generate_circle_points(center_x, center_y, radius, ax, num_points=100):\n","    # Calculate aspect ratio\n","    aspect_ratio = ax.get_data_ratio()\n","\n","    # Adjust the radius for the x and y coordinates\n","    radius_x = radius\n","    radius_y = radius * 1.3 * aspect_ratio\n","\n","    # Generate points for the circle\n","    theta = np.linspace(0, 2 * np.pi, num_points)\n","    x_points = center_x + radius_x * np.cos(theta)\n","    y_points = center_y + radius_y * np.sin(theta)\n","\n","    return x_points, y_points"]},{"cell_type":"code","execution_count":97,"metadata":{"executionInfo":{"elapsed":571,"status":"ok","timestamp":1706572815840,"user":{"displayName":"Cui Ding","userId":"13036128299667319583"},"user_tz":-60},"id":"VEQTmYOKUcgP"},"outputs":[],"source":["import textwrap\n","import matplotlib.patches as mpatches\n","import matplotlib.lines as mlines\n","import matplotlib.patches as mpatches\n","from matplotlib.legend_handler import HandlerPatch, HandlerLine2D\n","from matplotlib.legend_handler import HandlerBase\n","\n","# Define new colors\n","soft_blue = (100/255, 149/255, 237/255)  # Cornflower Blue\n","dark_blue = (70/255, 130/255, 180/255)   # Steel Blue\n","light_grey = (220/255, 220/255, 220/255) # Gainsboro\n","light_purple = (237/255, 239/255, 248/255)\n","subtle_orange = (255/255, 165/255, 0/255) # Orange\n","subtle_grey = (200/255, 200/255, 200/255)\n","bright_green = (0/255, 255/255, 0/255)\n","bright_orange = (255/255, 165/255, 0/255)\n","Magenta = (255/255, 0/255, 255/255)\n","\n","class CustomAssociationHandler(HandlerBase):\n","    def create_artists(self, legend, orig_handle, xdescent, ydescent, width, height, fontsize, trans):\n","        # Height for the patch (half of the total height)\n","        patch_height = height // 2\n","\n","        # Create patch (upper half)\n","        patch = mpatches.Rectangle([xdescent, ydescent + patch_height], width, patch_height,\n","                                   color=light_purple, alpha=0.5, transform=trans)\n","\n","        # Create line (bottom of the patch)\n","        line_y_position = ydescent\n","        line = mlines.Line2D([xdescent, xdescent + width], [line_y_position, line_y_position],\n","                             color=subtle_grey, linestyle='dashed', linewidth=1, transform=trans)\n","\n","        return [patch, line]"]},{"cell_type":"code","execution_count":98,"metadata":{"executionInfo":{"elapsed":509,"status":"ok","timestamp":1706572825790,"user":{"displayName":"Cui Ding","userId":"13036128299667319583"},"user_tz":-60},"id":"jkfDtripf5R_"},"outputs":[],"source":["def visualization_multi(grouped_df_cond, grouped_dff_cond, grouped_association_data, grouped_saccade_data, start_index, fig, axes):\n","  \"\"\"\n","  plot the reading path with associations and regressions marked.\n","  \"\"\"\n","  title_fontsize = 8\n","  word_fontsize = 7\n","  tick_fontsize = 7\n","  num_rows = len(axes)\n","\n","  # Convert the grouped object to a list for easier indexing\n","  grouped_list = list(grouped_df_cond)\n","  wrapper = textwrap.TextWrapper(width=180)\n","\n","  for i in range(num_rows):\n","      group_index = start_index + i\n","      if group_index < len(grouped_list):\n","          para_nr, group_cond = grouped_list[group_index]\n","          time = group_cond['t'].tolist()[:-1]\n","          x = group_cond['x'].tolist()[:-1]\n","          y = group_cond['y'].tolist()[:-1]\n","\n","          ax = axes[i]  # Simplified indexing\n","\n","          # Plot x and y on the subplot\n","          ax.plot(time, x)\n","          ax.plot(time, y)\n","\n","      wrapped_text = wrapper.fill(text=group_cond['text'].iloc[0])\n","      if (group_cond['correct'] == 1).all():\n","          ax.set_title(f\"Trial {group_cond['trial_id'].iloc[0]}: {wrapped_text}\", color='black', fontsize=title_fontsize)\n","      else:\n","          ax.set_title(f\"Trial {group_cond['trial_id'].iloc[0]}: {wrapped_text}\", color='red', fontsize=title_fontsize)\n","\n","      if para_nr in grouped_dff_cond.groups:\n","          group_dff_cond = grouped_dff_cond.get_group(para_nr)\n","          for index, f in group_dff_cond.iterrows():\n","              if f['end_t'] > time[-1]:\n","                  end_t = time[-1]\n","              else:\n","                  end_t = f['end_t']\n","              ax.axvspan(f['start_t'], end_t, color=light_purple)\n","              ax.axvline(end_t, color=subtle_grey, linestyle='dashed', linewidth=1)\n","              word = f['word']\n","              word_nr = f['word_nr'] + 1\n","              x_pos = (f['start_t'] + end_t) / 2\n","              y_pos = y[-1] + 250\n","              ax.text(x_pos, y_pos, f\"{word_nr} {word}\", rotation=90, ha='center', va='center', fontsize=word_fontsize)\n","\n","      if para_nr in grouped_association_data.groups:\n","          association_df = grouped_association_data.get_group(para_nr)\n","          for _, association in association_df.iterrows():\n","              center_x = association['start_t'] + association['duration'] / 2\n","              center_y = association['x_mean']\n","              radius = association['duration'] / 2\n","\n","              x_points, y_points = generate_circle_points(center_x, center_y, radius, ax)\n","              ax.plot(x_points, y_points, color=(245/255, 245/255, 245/255), linewidth=0.8)\n","              ax.fill(x_points, y_points, color=Magenta, alpha=0.5)\n","\n","\n","      if para_nr in grouped_saccade_data.groups:\n","          saccade_df = grouped_saccade_data.get_group(para_nr)\n","          band_width = 30\n","          for _, saccade in saccade_df.iterrows():\n","              start_time = saccade['start_t']\n","              end_time = saccade['end_t']\n","              # start_x = saccade['start_x']\n","              # end_x = saccade['end_x']\n","\n","              # Use boolean indexing to find the rows where 't' is between start_time and end_time\n","              subset = group_cond[(group_cond['t'] >= start_time) & (group_cond['t'] <= end_time)]\n","\n","              # Extract the relevant segments of 't' and 'x' from the subset\n","              subselected_time = subset['t'].tolist()\n","              subselected_x = subset['x'].tolist()\n","\n","              # Plotting the subselected segment\n","              if subselected_time and subselected_x:\n","\n","                  # Calculate the upper and lower boundaries of the band\n","                  upper_bound = [x_val + band_width / 2 for x_val in subselected_x]\n","                  lower_bound = [x_val - band_width / 2 for x_val in subselected_x]\n","\n","                  # Plot the band\n","                  ax.fill_between(subselected_time, lower_bound, upper_bound, color=bright_green, alpha=0.5)\n","\n","      association_circle = mlines.Line2D([], [], color=Magenta, marker='o', markersize=5, label='association', linestyle='None')\n","      saccade_patch = mpatches.Patch(color=bright_green, alpha=0.5, label='Saccade')\n","      horizontal_line = mlines.Line2D([], [], color=dark_blue, label='Horizontal Movement')\n","      vertical_line = mlines.Line2D([], [], color=(222/255, 154/255, 96/255), label='Vertical Movement')\n","      association_patch = mpatches.Patch(color=light_purple, alpha=0.5)\n","      association_line = mlines.Line2D([], [], color=subtle_grey, linestyle='dashed', linewidth=1)\n","\n","      #Create a legend for the plot\n","      legend_elements = [horizontal_line, vertical_line, (association_patch, association_line), association_circle, saccade_patch]\n","\n","      legend_labels = ['Horizontal Movement', 'Vertical Movement', 'Association', 'association', 'Saccade']\n","\n","      # Adjust y-axis limits, set labels, and add the legend\n","      ax.set_xlabel('time(ms)')\n","      ax.set_ylabel('position in pixels')\n","      ax.legend(handles=legend_elements, labels=legend_labels,\n","                handler_map={association_line: CustomAssociationHandler()},\n","                loc='upper left', fontsize=6)\n","\n","      custom_ticks = np.arange(0, time[-1], 1000)\n","      ax.set_xticks(custom_ticks)\n","      ax.tick_params(axis='both', which='both', labelsize=tick_fontsize)\n","\n","  # Adjust spacing between subplots\n","  plt.subplots_adjust(hspace=0.6, wspace=0.2)\n","  plt.tight_layout()\n","\n","  # Show the plot\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"VpkcComL4mAr"},"source":["## Plot Association-association-Saccade for multiple files\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zp62OEQL4lNM"},"outputs":[],"source":["trial_data = Path('/content/drive/MyDrive/MoTR/trial_data/provo_items.tsv')\n","\n","reading_data_path = Path(f'/content/drive/MyDrive/MoTR/provo_plots_test/provo_cleaned_raw_association_not_merged')\n","\n","# Iterate over each file in the directory\n","for file_path in reading_data_path.iterdir():\n","    # Check if it's a file and not a directory\n","    if file_path.is_file():\n","        print(f\"Processing file: {file_path}\")\n","        reader = str(file_path.stem)\n","        print(f\"Reader: {reader}\")\n","\n","        association_data = Path(f'/content/drive/MyDrive/MoTR/provo_associations/2_provo_association_160ms/{reader}_clean.csv')\n","        # Check if association_data exists\n","        if not association_data.exists():\n","            print(f\"Association data for {reader} does not exist. Skipping...\")\n","            continue\n","\n","\n","        association_data = Path(f'/content/drive/MyDrive/MoTR/provo_2024/associations_2024/associations_{reader}.csv')\n","        saccade_data = Path(f'/content/drive/MyDrive/MoTR/provo_2024/Saccades_2024/Saccades_{reader}.csv')\n","\n","        dfw = pd.read_csv(file_path)\n","        dff = pd.read_csv(association_data)\n","        dft = pd.read_csv(trial_data, sep='\\t')\n","\n","        df_associations = pd.read_csv(association_data)\n","        df_saccades = pd.read_csv(saccade_data)\n","        # Group association and saccade data by 'para_nr'\n","        grouped_association_data = df_associations.groupby('trial_id')\n","        grouped_saccade_data = df_saccades.groupby('trial_id')\n","\n","        # Create a PDF file to save the plots\n","        pdf_filename = f'/content/drive/MyDrive/MoTR/provo_2024/AFS_Plots/AFS_plot_{reader}.pdf'\n","        pdf_pages = pdf.PdfPages(pdf_filename)\n","\n","        # # Check if association_data exists\n","        # if Path(pdf_filename).exists():\n","        #     print(f\"Plot for {reader} already exist. Skipping...\")\n","        #     continue\n","\n","        condition = 1\n","        dfw_cond = dfw[dfw['cond_id'] == condition]\n","        dft_cond = dft[dft['condition_id'] == condition]\n","        dff_cond = dff[dff['cond_id'] == condition]\n","        df_associations_cond = df_associations[df_associations['cond_id'] == condition]\n","        df_saccades_cond = df_saccades[df_saccades['cond_id'] == condition]\n","\n","        # todo: add trial id to association files\n","        dff_cond = dff_cond[['para_nr', 'word_nr', 'word', 'duration', 'start_t', 'end_t', 'x_mean', 'y_mean']]\n","\n","        dft_cond = dft_cond[['experiment_id', 'condition_id', 'item_id', 'text', 'response_true']]\n","        new_column_name = {'experiment_id': 'expr_id', 'condition_id': 'cond_id', 'item_id': 'para_nr'}\n","        dft_cond = dft_cond.rename(columns=new_column_name)\n","\n","        df_cond = pd.merge(dfw_cond, dft_cond, on=['expr_id', 'cond_id', 'para_nr'])\n","        df_cond = df_cond.assign(correct=0)\n","        df_cond.loc[df_cond['response_true'] == df_cond['response'], 'correct'] = 1\n","        df_cond = df_cond[['cond_id', 'trial_id', 'para_nr', 'word_nr', 'word', 'text', 't', 'x', 'y', 'response', 'response_true', 'correct']]\n","        # try to get trial_id for association df.\n","        trial_id_mapping = df_cond.drop_duplicates('para_nr').set_index('para_nr')['trial_id'].to_dict()\n","\n","        # Map the trial_id to dff_cond using the created dictionary\n","        dff_cond['trial_id'] = dff_cond['para_nr'].map(trial_id_mapping)\n","\n","        grouped_df_cond = df_cond.groupby('trial_id')\n","        grouped_dff_cond = dff_cond.groupby('trial_id')\n","\n","        num_rows = 3\n","        num_cols = 1\n","\n","        for i in range(0, len(grouped_df_cond), num_rows):\n","            fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 16))\n","            plt.subplots_adjust(hspace=0.3)\n","\n","            visualization_multi(grouped_df_cond, grouped_dff_cond, grouped_association_data, grouped_saccade_data, i, fig, axes)\n","            pdf_pages.savefig(fig)\n","\n","        # Close the PDF file\n","        pdf_pages.close()\n","\n","        print(f'Plots saved to {pdf_filename}.')\n","\n","\n",""]},{"cell_type":"code","source":["readers = ['3']\n","\n","for reader in readers:\n","  trial_data = Path('/content/drive/MyDrive/MoTR/trial_data/provo_items.tsv')\n","\n","  reading_data = Path(f'/content/drive/MyDrive/MoTR/provo_plots_test/provo_cleaned_raw_association_not_merged/reader_{reader}.csv')\n","\n","  association_data = Path(f'/content/drive/MyDrive/MoTR/provo_associations/2_provo_association_160ms/reader_{reader}_clean.csv')\n","  association_data = Path(f'/content/drive/MyDrive/MoTR/provo_plots_test/associations_reader_{reader}.csv')\n","  saccade_data = Path(f'/content/drive/MyDrive/MoTR/provo_plots_test/Saccades_reader_{reader}.csv')\n","\n","  dfw = pd.read_csv(reading_data)\n","  dff = pd.read_csv(association_data)\n","  dft = pd.read_csv(trial_data, sep='\\t')\n","\n","  df_associations = pd.read_csv(association_data)\n","  df_saccades = pd.read_csv(saccade_data)\n","  grouped_association_data = df_associations.groupby('trial_id')\n","  grouped_saccade_data = df_saccades.groupby('trial_id')\n","\n","\n","  # Create a PDF file to save the plots\n","  pdf_filename = f'/content/drive/MyDrive/MoTR/provo_plots_test/reader_{reader}_plots3.pdf'\n","  pdf_pages = pdf.PdfPages(pdf_filename)\n","\n","  condition = 1\n","  dfw_cond = dfw[dfw['cond_id'] == condition]\n","  dft_cond = dft[dft['condition_id'] == condition]\n","  dff_cond = dff[dff['cond_id'] == condition]\n","  df_associations_cond = df_associations[df_associations['cond_id'] == condition]\n","  df_saccades_cond = df_saccades[df_saccades['cond_id'] == condition]\n","\n","  # todo: add trial id to association files\n","  dff_cond = dff_cond[['para_nr', 'word_nr', 'word', 'duration', 'start_t', 'end_t', 'x_mean', 'y_mean']]\n","\n","  dft_cond = dft_cond[['experiment_id', 'condition_id', 'item_id', 'text', 'response_true']]\n","  new_column_name = {'experiment_id': 'expr_id', 'condition_id': 'cond_id', 'item_id': 'para_nr'}\n","  dft_cond = dft_cond.rename(columns=new_column_name)\n","\n","  df_cond = pd.merge(dfw_cond, dft_cond, on=['expr_id', 'cond_id', 'para_nr'])\n","  df_cond = df_cond.assign(correct=0)\n","  df_cond.loc[df_cond['response_true'] == df_cond['response'], 'correct'] = 1\n","  df_cond = df_cond[['cond_id', 'trial_id', 'para_nr', 'word_nr', 'word', 'text', 't', 'x', 'y', 'response', 'response_true', 'correct']]\n","  # try to get trial_id for association df.\n","  trial_id_mapping = df_cond.drop_duplicates('para_nr').set_index('para_nr')['trial_id'].to_dict()\n","\n","  # Map the trial_id to dff_cond using the created dictionary\n","  dff_cond['trial_id'] = dff_cond['para_nr'].map(trial_id_mapping)\n","\n","\n","  # df_associations_cond = df_associations_cond.sort_values(by='trial_id')\n","  # df_saccades_cond = df_saccades_cond.sort_values(by='trial_id')\n","  # df_cond = df_cond.sort_values(by='trial_id')\n","  # dff_cond = dff_cond.sort_values(by='trial_id')\n","\n","  # Group association and saccade data by 'para_nr'\n","  grouped_df_cond = df_cond.groupby('trial_id')\n","  grouped_dff_cond = dff_cond.groupby('trial_id')\n","\n","  num_rows = 3\n","  num_cols = 1\n","\n","  for i in range(0, len(grouped_df_cond), num_rows):\n","      fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 16))\n","      plt.subplots_adjust(hspace=0.3)\n","\n","      visualization_multi(grouped_df_cond, grouped_dff_cond, grouped_association_data, grouped_saccade_data, i, fig, axes)\n","      pdf_pages.savefig(fig)\n","\n","  # Close the PDF file\n","  pdf_pages.close()\n","\n","  print(f'Plots saved to {pdf_filename}.')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1fSqndbJRkhm7np7Z9x7Q84vaWwkNZNh_"},"id":"UklGeMScOaeg","executionInfo":{"status":"ok","timestamp":1706571300592,"user_tz":-60,"elapsed":17824,"user":{"displayName":"Cui Ding","userId":"13036128299667319583"}},"outputId":"ecc1a2c8-5f8d-4e06-88ff-a8e21e7b4a13"},"execution_count":89,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dSZ_c1hpf5Pd"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyOaIJGUgRI4vWHsegSoAVhB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}