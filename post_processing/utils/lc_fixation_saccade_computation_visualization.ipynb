{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29776,"status":"ok","timestamp":1706554364611,"user":{"displayName":"Cui Ding","userId":"13036128299667319583"},"user_tz":-60},"id":"walG3g8bFiUH","outputId":"ce1ca5eb-c349-485e-e731-f88f26c33eba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNdYjemmWpST"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import matplotlib.backends.backend_pdf as pdf\n","from matplotlib.patches import Circle"]},{"cell_type":"code","source":[],"metadata":{"id":"0TKQ-AHFeDgj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clean raw data by deleting the noise before reading; do not merge samples to associations/associations etc."],"metadata":{"id":"l6jDJaLleE2E"}},{"cell_type":"code","source":["raw = Path(f'/content/drive/MyDrive/MoTR/local_coherence_divided_by_reader').glob('*.csv')\n","out = Path(f'/content/drive/MyDrive/MoTR/local_coherence_cleaned_raw_association_not_merged')\n","\n","\n","for f in raw:\n","  df = pd.read_csv(f)\n","  # # If we want to add trial id and make sure submission_id is the same as reader number.\n","  # reader_nr = f.stem.split('_')[1]\n","\n","  # # Assign this reader number to the 'submission_id' column\n","  # df['submission_id'] = reader_nr\n","  # # Create a dictionary to map para_nr to trial sequence number\n","  # trial_sequence = {}\n","  # current_trial = 0\n","  # for item_id in df['ItemId'].unique():\n","  #     trial_sequence[item_id] = current_trial\n","  #     current_trial += 1\n","\n","  # # Add a new column 'trial_nr' to dfw using the trial_sequence mapping\n","  # df['trial_id'] = df['ItemId'].map(trial_sequence)\n","\n","  # # Write the modified DataFrame to a new file in the same directory with the same file name\n","  # output_file = f.parent / f.name\n","  # df.to_csv(output_file, index=False)\n","\n","  df.loc[:, 'sbm_id'] = df['submission_id'].astype(str)\n","  df.loc[:, 'expr_id'] = df['Experiment'].astype(int)\n","  df.loc[:, 'cond_id'] = df['Condition'].astype(int)\n","  df.loc[:, 'para_nr'] = df['ItemId'].astype(int)\n","  df.loc[:, 'word_nr'] = df['Index'].astype(int)\n","  df.loc[:, 'word'] = df['Word'].astype(str)\n","  df.loc[:, 't'] = df['responseTime'].astype(int)\n","  df.loc[:, 'x'] = df['mousePositionX'].astype(int)\n","  df.loc[:, 'y'] = df['mousePositionY'].astype(int)\n","  df.loc[:, 'wb'] = df['wordPositionBottom'].astype(str)\n","  df.loc[:, 'wt'] = df['wordPositionTop'].astype(str)\n","  df.loc[:, 'wl'] = df['wordPositionLeft'].astype(str)\n","  df.loc[:, 'wr'] = df['wordPositionRight'].astype(str)\n","  df.loc[:, 'response'] = df['response'].astype(str)\n","\n","  dfw = df[['sbm_id', 'expr_id', 'cond_id', 'trial_id', 'para_nr', 'word_nr', 'word', 't',\n","            'x', 'y', 'wb', 'wt', 'wl', 'wr', 'response']]\n","\n","  grouped_df = dfw.groupby(['cond_id', 'para_nr'])\n","  filtered_df = pd.DataFrame()\n","\n","  for name, group in grouped_df:\n","      filtered_group = group[group['word_nr'].isin([0, 1, 2, 3])]\n","      if not filtered_group.empty:\n","        first_idx = filtered_group.index[0]\n","        # Delete all rows before the first row with 'word_nr' in [0, 1, 2, 3]\n","        group = group.loc[first_idx:]\n","\n","        # Concatenate the filtered group to the filtered_df DataFrame\n","        filtered_df = pd.concat([filtered_df, group], ignore_index=True)\n","\n","\n","  filtered_df = filtered_df.reset_index(drop=True)\n","  filtered_df.to_csv(f'{out}/{f.stem}.csv')"],"metadata":{"id":"B7mTzkQzs0CI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EGiiVGuBBGn8"},"source":["## Define a Velocity-based association detection function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNtQmGf_U048"},"outputs":[],"source":["def most_frequent(series):\n","    \"\"\"\n","    Determine the most frequent value in a Pandas Series.\n","    :Parameters series: The Pandas series for which the mode is to be calculated.\n","    \"\"\"\n","    if series.mode().empty:\n","        return \"%2c%\"\n","    mode_value = series.mode()[0]\n","    return mode_value if not pd.isna(mode_value) else \"%2c%\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FeKffpD5BTf8"},"outputs":[],"source":["def ivt(gaze_data, vel_thres, dur_thres_fix_low, dur_thres_fix_high, accer_thres, dur_thres_sac):\n","    \"\"\"\n","    Identify associations and saccades in gaze data using the I-VT algorithm.\n","\n","    :param gaze_data: DataFrame with columns ['x', 'y', 't'] for gaze points.\n","    :param vel_thres: Velocity threshold to differentiate saccades from associations.\n","    :param dur_thres_fix_low: Duration threshold to confirm associations (in milliseconds).\n","    :param dur_thres_fix_high: Duration threshold to confirm associations (in milliseconds).\n","    :param accer_thres: Accerelation threshold to detect potential saccades.\n","    :param dur_thres_sac: Duration threshold to confirm saccades (in milliseconds).\n","    :return: DataFrame with an additional 'type' column labeling each point as 'association' or 'saccade'.\n","    \"\"\"\n","    # gaze_data = gaze_data.dropna()\n","\n","    # Calculate distances and velocities\n","    dx = np.diff(gaze_data['x'])\n","    dy = np.diff(gaze_data['y'])\n","    dt = np.diff(gaze_data['t'])\n","\n","    # # Prepend 0 to the differences\n","    dx = np.insert(dx, 0, 0)\n","    dy = np.insert(dy, 0, 0)\n","    dt = np.insert(dt, 0, 0)\n","\n","    # Avoid division by zero\n","    dt = dt.astype(float)\n","    dt[dt == 0] = 1e6\n","\n","    distances = np.sqrt(dx**2 + dy**2)\n","    velocities = distances / dt\n","    dv = np.diff(velocities)\n","    dv = np.insert(dv, 0, 0)\n","    acceleration = dv / dt\n","\n","    # Classify points as associations, saccades or slidings (association_vel + 0.05 px/ms).\n","\n","    gaze_data['type'] = np.where(\n","        (np.absolute(acceleration) < accer_thres) & (velocities < vel_thres),\n","    'association',\n","    np.where(\n","        (np.absolute(acceleration) < accer_thres) & (velocities < vel_thres + 0.05),\n","        'sliding',\n","        'saccade'\n","      )\n","    )\n","    gaze_data['velocities'] = velocities\n","    gaze_data['acceleration'] = acceleration\n","\n","    # Exclude rows where word_nr is -100 (for comprehension question)\n","    gaze_data = gaze_data[gaze_data['word_nr'] != -100]\n","\n","    # Group consecutive points and filter based on duration\n","    gaze_data['group'] = (gaze_data['type'] != gaze_data['type'].shift()).cumsum()\n","    associations = gaze_data[gaze_data['type'] == 'association'].groupby('group').filter(lambda x: dur_thres_fix_low <= (x['t'].iloc[-1] - x['t'].iloc[0]) <= dur_thres_fix_high)\n","    saccades = gaze_data[gaze_data['type'] == 'saccade'].groupby('group').filter(lambda x: (x['t'].iloc[-1] - x['t'].iloc[0]) >= dur_thres_sac)\n","\n","\n","    association_centroid = associations.groupby('group').agg(\n","    sbm_id=('sbm_id', 'first'),\n","    expr_id=('expr_id', 'first'),\n","    cond_id=('cond_id', 'first'),\n","    trial_id = ('trial_id', 'first'),\n","    para_nr=('para_nr', 'first'),\n","    word_nr=('word_nr', most_frequent),\n","    word=('word', most_frequent),\n","    x_mean=('x', 'mean'),\n","    y_mean=('y', 'mean'),\n","    start_t=('t', 'first'),\n","    end_t=('t', 'last')\n","    )\n","    association_centroid['duration'] = association_centroid['end_t'] - association_centroid['start_t']\n","    # Filter out groups where word_nr is -1 or word is null (fix at blank area)\n","    association_centroid = association_centroid[(association_centroid['word_nr'] != -1) & (association_centroid['word'] != \"%2c%\")]\n","\n","    # Calculate statistics for each saccade group\n","    saccade_stats = saccades.groupby('group').agg(\n","        sbm_id=('sbm_id', 'first'),\n","        expr_id=('expr_id', 'first'),\n","        cond_id=('cond_id', 'first'),\n","        trial_id = ('trial_id', 'first'),\n","        para_nr=('para_nr', 'first'),\n","        mean_x=('x', 'mean'),\n","        mean_y=('y', 'mean'),\n","        start_x=('x', 'first'),\n","        end_x=('x', 'last'),\n","        start_y=('y', 'first'),\n","        end_y=('y', 'last'),\n","        start_t=('t', 'first'),\n","        end_t=('t', 'last'),\n","        # mean_velocity=('velocity', 'mean'),\n","        # mean_acceleration=('acceleration', 'mean')\n","    )\n","    saccade_stats['duration'] = saccade_stats['end_t'] - saccade_stats['start_t']\n","\n","    return association_centroid, saccade_stats, gaze_data.iloc[:, 1:-1]"]},{"cell_type":"markdown","metadata":{"id":"yFUKplyZqeNO"},"source":["## Read in all the files and get associations, saccades"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJrfGb9AqUj5"},"outputs":[],"source":["vel_thres = 0.1\n","dur_thres_fix_low = 160\n","dur_thres_fix_high = 4000\n","accer_thres = 0.01\n","dur_thres_sac = 80\n","\n","reading_data_path = Path(f'/content/drive/MyDrive/MoTR/local_coherence_cleaned_raw_association_not_merged')\n","\n","# Iterate over each file in the directory\n","for file_path in reading_data_path.iterdir():\n","    # Check if it's a file and not a directory\n","    if file_path.is_file():\n","        print(f\"Processing file: {file_path}\")\n","        reader = str(file_path).split('/')[-1][:-4]\n","        print(f\"Reader: {reader}\")\n","        reading_data = pd.read_csv(file_path)\n","        reading_data.rename(columns={\n","        'submission_id': 'sbm_id',\n","        'Experiment': 'expr_id',\n","        'Condition': 'cond_id',\n","        'ItemId': 'para_nr',\n","        'Index': 'word_nr',\n","        'Word': 'word',\n","        'responseTime': 't',\n","        'mousePositionX': 'x',\n","        'mousePositionY': 'y',\n","        # Uncomment and rename other columns if needed\n","        # 'wordPositionBottom': 'wb',\n","        # 'wordPositionTop': 'wt',\n","        # 'wordPositionLeft': 'wl',\n","        # 'wordPositionRight': 'wr',\n","        'response': 'response'\n","        }, inplace=True)\n","\n","        all_associations = []\n","        all_saccades = []\n","        all_gaze = []\n","\n","        for para_nr in reading_data['para_nr'].unique():\n","            item_data = reading_data[reading_data['para_nr'] == para_nr]\n","            # Extract necessary information\n","            associations, saccades, gaze_data = ivt(item_data, vel_thres, dur_thres_fix_low, dur_thres_fix_high, accer_thres, dur_thres_sac)\n","\n","            # Append associations and saccades to all_associations and all_saccades\n","            all_associations.append(associations)\n","            all_saccades.append(saccades)\n","            all_gaze.append(gaze_data)\n","\n","        # Combine all item results into single DataFrames\n","        all_associations_df = pd.concat(all_associations, ignore_index=True)\n","        all_saccades_df = pd.concat(all_saccades, ignore_index=True)\n","        all_gaze_df = pd.concat(all_gaze, ignore_index=True)\n","\n","        # Write to CSV\n","        all_associations_df.to_csv(f'/content/drive/MyDrive/MoTR/local_coherence_2024/associations_2024/associations_{reader}.csv', index=False)\n","        all_saccades_df.to_csv(f'/content/drive/MyDrive/MoTR/local_coherence_2024/Saccades_2024/Saccades_{reader}.csv', index=False)\n","        all_gaze_df.to_csv(f'/content/drive/MyDrive/MoTR/local_coherence_2024/associations_Saccades_Slidings/associations_Saccades_Slidings_unmerged_{reader}.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"UT8ua8REmGDP"},"source":["## Define a function, take two dfs which has been grouped over cond as arguement."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvD5hPBOt_nt"},"outputs":[],"source":["vel_thres = 0.1\n","dur_thres_fix = 160\n","accer_thres = 0.01\n","dur_thres_sac = 80\n","\n","readers = ['70', '72']\n","\n","for reader in readers:\n","    reading_data_path = Path(f'/content/drive/MyDrive/MoTR/local_coherence_cleaned_raw_association_not_merged/reader_{reader}.csv')\n","    reading_data = pd.read_csv(reading_data_path)\n","    reading_data.rename(columns={\n","    'submission_id': 'sbm_id',\n","    'Experiment': 'expr_id',\n","    'Condition': 'cond_id',\n","    'ItemId': 'para_nr',\n","    'Index': 'word_nr',\n","    'Word': 'word',\n","    'responseTime': 't',\n","    'mousePositionX': 'x',\n","    'mousePositionY': 'y',\n","    # Uncomment and rename other columns if needed\n","    # 'wordPositionBottom': 'wb',\n","    # 'wordPositionTop': 'wt',\n","    # 'wordPositionLeft': 'wl',\n","    # 'wordPositionRight': 'wr',\n","    'response': 'response'\n","    }, inplace=True)\n","\n","    all_associations = []\n","    all_saccades = []\n","    all_gaze = []\n","\n","    for para_nr in reading_data['para_nr'].unique():\n","        item_data = reading_data[reading_data['para_nr'] == para_nr]\n","\n","        # Extract necessary information\n","\n","        associations, saccades, gaze_data = ivt(item_data, vel_thres, dur_thres_fix, accer_thres, dur_thres_sac)\n","\n","        # Append associations and saccades to all_associations and all_saccades\n","        all_associations.append(associations)\n","        all_saccades.append(saccades)\n","        all_gaze.append(gaze_data)\n","\n","    # Combine all item results into single DataFrames\n","    all_associations_df = pd.concat(all_associations, ignore_index=True)\n","    all_saccades_df = pd.concat(all_saccades, ignore_index=True)\n","    all_gaze_df = pd.concat(all_gaze, ignore_index=True)\n","\n","    # Write to CSV\n","    all_associations_df.to_csv(f'/content/drive/MyDrive/MoTR/local_coherence_plots_2024/associations_reader_{reader}.csv', index=False)\n","    all_saccades_df.to_csv(f'/content/drive/MyDrive/MoTR/local_coherence_plots_2024/Saccades_reader_{reader}.csv', index=False)\n","    all_gaze_df.to_csv(f'/content/drive/MyDrive/MoTR/local_coherence_plots_2024/associations_Saccades_Slidings_unmerged_reader_{reader}.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bGJkwhb1l8i"},"outputs":[],"source":["def generate_circle_points(center_x, center_y, radius, ax, num_points=100):\n","    # Calculate aspect ratio\n","    aspect_ratio = ax.get_data_ratio()\n","\n","    # Adjust the radius for the x and y coordinates\n","    radius_x = radius\n","    radius_y = radius * 1.3 * aspect_ratio\n","\n","    # Generate points for the circle\n","    theta = np.linspace(0, 2 * np.pi, num_points)\n","    x_points = center_x + radius_x * np.cos(theta)\n","    y_points = center_y + radius_y * np.sin(theta)\n","\n","    return x_points, y_points"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEQTmYOKUcgP"},"outputs":[],"source":["import matplotlib.patches as mpatches\n","import matplotlib.lines as mlines\n","import matplotlib.patches as mpatches\n","from matplotlib.legend_handler import HandlerPatch, HandlerLine2D\n","from matplotlib.legend_handler import HandlerBase\n","\n","# Define new colors\n","soft_blue = (100/255, 149/255, 237/255)  # Cornflower Blue\n","dark_blue = (70/255, 130/255, 180/255)   # Steel Blue\n","light_grey = (220/255, 220/255, 220/255) # Gainsboro\n","light_purple = (237/255, 239/255, 248/255)\n","subtle_orange = (255/255, 165/255, 0/255) # Orange\n","subtle_grey = (200/255, 200/255, 200/255)\n","bright_green = (0/255, 255/255, 0/255)\n","bright_orange = (255/255, 165/255, 0/255)\n","\n","class CustomAssociationHandler(HandlerBase):\n","    def create_artists(self, legend, orig_handle, xdescent, ydescent, width, height, fontsize, trans):\n","        # Height for the patch (half of the total height)\n","        patch_height = height // 2\n","\n","        # Create patch (upper half)\n","        patch = mpatches.Rectangle([xdescent, ydescent + patch_height], width, patch_height,\n","                                   color=light_purple, alpha=0.5, transform=trans)\n","\n","        # Create line (bottom of the patch)\n","        line_y_position = ydescent\n","        line = mlines.Line2D([xdescent, xdescent + width], [line_y_position, line_y_position],\n","                             color=subtle_grey, linestyle='dashed', linewidth=1, transform=trans)\n","\n","        return [patch, line]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkfDtripf5R_"},"outputs":[],"source":["def visualization_multi(grouped_df_cond, grouped_dff_cond, grouped_association_data, grouped_saccade_data, fig, axes):\n","  \"\"\"\n","  plot the reading path with associations and regressions marked.\n","  \"\"\"\n","  title_fontsize = 8\n","  word_fontsize = 7\n","  tick_fontsize = 7\n","\n","  for i, group in enumerate(grouped_df_cond):\n","      para_nr = group[0]\n","      group_cond = group[1]\n","      time = group_cond['t'].tolist()[:-1]\n","      x = group_cond['x'].tolist()[:-1]\n","      y = group_cond['y'].tolist()[:-1]\n","\n","      # Get the corresponding subplot based on the index\n","      row = i // num_cols\n","      col = i % num_cols\n","      ax = axes[row, col]\n","\n","      # Plot x and y on the subplot\n","      ax.plot(time, x)\n","      ax.plot(time, y)\n","\n","      if (group_cond['correct'] == 1).all():\n","          ax.set_title(f\"Condition {group_cond['cond_id'].iloc[0]}:\\n{group_cond['text'].iloc[0]}\", color='black', fontsize=title_fontsize)\n","      else:\n","          ax.set_title(f\"Condition {group_cond['cond_id'].iloc[0]}:\\n{group_cond['text'].iloc[0]}\", color='red', fontsize=title_fontsize)\n","\n","      if para_nr in grouped_dff_cond.groups:\n","          group_dff_cond = grouped_dff_cond.get_group(para_nr)\n","          for index, f in group_dff_cond.iterrows():\n","              if f['end_t'] > time[-1]:\n","                  end_t = time[-1]\n","              else:\n","                  end_t = f['end_t']\n","              ax.axvspan(f['start_t'], end_t, color=light_purple)\n","              ax.axvline(end_t, color=subtle_grey, linestyle='dashed', linewidth=1)\n","              word = f['word']\n","              word_nr = f['word_nr'] + 1\n","              x_pos = (f['start_t'] + end_t) / 2\n","              y_pos = y[-1] + 250\n","              ax.text(x_pos, y_pos, f\"{word_nr} {word}\", rotation=90, ha='center', va='center', fontsize=word_fontsize)\n","\n","      if para_nr in grouped_association_data.groups:\n","          association_df = grouped_association_data.get_group(para_nr)\n","          for _, association in association_df.iterrows():\n","              center_x = association['start_t'] + association['duration'] / 2\n","              center_y = association['x_mean']\n","              radius = association['duration'] / 2\n","\n","              x_points, y_points = generate_circle_points(center_x, center_y, radius, ax)\n","              ax.plot(x_points, y_points, color=(245/255, 245/255, 245/255), linewidth=0.8)\n","              ax.fill(x_points, y_points, color=soft_blue, alpha=0.5)\n","\n","\n","      if para_nr in grouped_saccade_data.groups:\n","          saccade_df = grouped_saccade_data.get_group(para_nr)\n","          band_width = 30\n","          for _, saccade in saccade_df.iterrows():\n","              start_time = saccade['start_t']\n","              end_time = saccade['end_t']\n","              # start_x = saccade['start_x']\n","              # end_x = saccade['end_x']\n","\n","              # Use boolean indexing to find the rows where 't' is between start_time and end_time\n","              subset = group_cond[(group_cond['t'] >= start_time) & (group_cond['t'] <= end_time)]\n","\n","              # Extract the relevant segments of 't' and 'x' from the subset\n","              subselected_time = subset['t'].tolist()\n","              subselected_x = subset['x'].tolist()\n","\n","              # Plotting the subselected segment\n","              if subselected_time and subselected_x:\n","\n","                  # Calculate the upper and lower boundaries of the band\n","                  upper_bound = [x_val + band_width / 2 for x_val in subselected_x]\n","                  lower_bound = [x_val - band_width / 2 for x_val in subselected_x]\n","\n","                  # Plot the band\n","                  ax.fill_between(subselected_time, lower_bound, upper_bound, color=bright_green, alpha=0.5)\n","\n","      association_circle = mlines.Line2D([], [], color=soft_blue, marker='o', markersize=5, label='association', linestyle='None')\n","      saccade_patch = mpatches.Patch(color=bright_green, alpha=0.5, label='Saccade')\n","      horizontal_line = mlines.Line2D([], [], color=dark_blue, label='Horizontal Movement')\n","      vertical_line = mlines.Line2D([], [], color=(222/255, 154/255, 96/255), label='Vertical Movement')\n","      association_patch = mpatches.Patch(color=light_purple, alpha=0.5)\n","      association_line = mlines.Line2D([], [], color=subtle_grey, linestyle='dashed', linewidth=1)\n","\n","      # Create a legend for the plot\n","      legend_elements = [horizontal_line, vertical_line, (association_patch, association_line), association_circle, saccade_patch]\n","\n","      legend_labels = ['Horizontal Movement', 'Vertical Movement', 'Association', 'association', 'Saccade']\n","\n","      # Adjust y-axis limits, set labels, and add the legend\n","      ax.set_xlabel('time(ms)')\n","      ax.set_ylabel('position in pixels')\n","      ax.legend(handles=legend_elements, labels=legend_labels,\n","                handler_map={association_line: CustomAssociationHandler()},\n","                loc='upper left', fontsize=6)\n","      # ax.legend(['horizontal movement', 'vertical movement', 'association', 'association', 'Saccade'], loc='upper left', fontsize=6)\n","\n","      custom_ticks = np.arange(0, time[-1], 1000)\n","      ax.set_xticks(custom_ticks)\n","      ax.tick_params(axis='both', which='both', labelsize=tick_fontsize)\n","\n","  # Adjust spacing between subplots\n","  # plt.subplots_adjust(hspace=0.6, wspace=0.2)\n","  # plt.tight_layout()\n","\n","  # Show the plot\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"VpkcComL4mAr"},"source":["## Plot Association-association-Saccade for multiple files\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zp62OEQL4lNM"},"outputs":[],"source":["trial_data = Path('/content/drive/MyDrive/MoTR/trial_data/localCoherence.tsv')\n","\n","reading_data_path = Path(f'/content/drive/MyDrive/MoTR/local_coherence_cleaned_raw_association_not_merged')\n","\n","# Iterate over each file in the directory\n","for file_path in reading_data_path.iterdir():\n","    # Check if it's a file and not a directory\n","    if file_path.is_file():\n","        print(f\"Processing file: {file_path}\")\n","        reader = str(file_path).split('/')[-1][:-4]\n","        print(f\"Reader: {reader}\")\n","\n","        association_data = Path(f'/content/drive/MyDrive/MoTR/local_coherence_associations/local_coherence_associations_f160/{reader}_merged_denoised.csv')\n","        # Check if association_data exists\n","        if not association_data.exists():\n","            print(f\"Association data for {reader} does not exist. Skipping...\")\n","            continue\n","\n","\n","        association_data = Path(f'/content/drive/MyDrive/MoTR/local_coherence_2024/associations_2024/associations_{reader}.csv')\n","        saccade_data = Path(f'/content/drive/MyDrive/MoTR/local_coherence_2024/Saccades_2024/Saccades_{reader}.csv')\n","\n","        dfw = pd.read_csv(file_path)\n","        dff = pd.read_csv(association_data)\n","        dft = pd.read_csv(trial_data, sep='\\t')\n","\n","        df_associations = pd.read_csv(association_data)\n","        df_saccades = pd.read_csv(saccade_data)\n","        # Group association and saccade data by 'para_nr'\n","        grouped_association_data = df_associations.groupby('para_nr')\n","        grouped_saccade_data = df_saccades.groupby('para_nr')\n","\n","        # Create a PDF file to save the plots\n","        pdf_filename = f'/content/drive/MyDrive/MoTR/local_coherence_2024/AFS_Plots/AFS_plot_{reader}.pdf'\n","        pdf_pages = pdf.PdfPages(pdf_filename)\n","\n","        # # Check if association_data exists\n","        # if Path(pdf_filename).exists():\n","        #     print(f\"Plot for {reader} already exist. Skipping...\")\n","        #     continue\n","\n","        for condition in [1, 2, 3, 4, 5, 6]:\n","          dfw_cond = dfw[dfw['cond_id'] == condition]\n","          dft_cond = dft[dft['condition_id'] == condition]\n","          dff_cond = dff[dff['cond_id'] == condition]\n","\n","          dff_cond = dff_cond[['para_nr', 'word_nr', 'word', 'duration', 'start_t', 'end_t', 'x_mean', 'y_mean']]\n","\n","          dft_cond = dft_cond[['experiment_id', 'condition_id', 'item_id', 'text', 'response_true']]\n","          new_column_name = {'experiment_id': 'expr_id', 'condition_id': 'cond_id', 'item_id': 'para_nr'}\n","          dft_cond = dft_cond.rename(columns=new_column_name)\n","\n","          df_cond = pd.merge(dfw_cond, dft_cond, on=['expr_id', 'cond_id', 'para_nr'])\n","          df_cond = df_cond.assign(correct=0)\n","          df_cond.loc[df_cond['response_true'] == df_cond['response'], 'correct'] = 1\n","          df_cond = df_cond[['cond_id', 'para_nr', 'word_nr', 'word', 'text', 't', 'x', 'y', 'response', 'response_true', 'correct']]\n","          grouped_df_cond = df_cond.groupby('para_nr')\n","          grouped_dff_cond = dff_cond.groupby('para_nr')\n","\n","          num_rows = 4\n","          num_cols = 2\n","          fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 16))\n","          plt.subplots_adjust(hspace=0.3)\n","          visualization_multi(grouped_df_cond, grouped_dff_cond, grouped_association_data, grouped_saccade_data, fig, axes)\n","\n","          # Save the current plot to the PDF file\n","          pdf_pages.savefig(fig)\n","\n","        # Close the PDF file\n","        pdf_pages.close()\n","\n","        print(f'Plots saved to {pdf_filename}.')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ATwoqUCcq21YK38EUK3Leo-5EEhlBwx5"},"executionInfo":{"elapsed":27677,"status":"ok","timestamp":1706531596857,"user":{"displayName":"Cui Ding","userId":"13036128299667319583"},"user_tz":-60},"id":"bvaS_iMxXAT3","outputId":"67f1233a-31db-4fa7-c9a8-a5656f6c470e"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["readers = ['70']\n","# int\n","# condition = 5\n","# int\n","# text = 20\n","for reader in readers:\n","  trial_data = Path('/content/drive/MyDrive/MoTR/trial_data/localCoherence.tsv')\n","\n","  reading_data = Path(f'/content/drive/MyDrive/MoTR/local_coherence_cleaned_raw_association_not_merged/reader_{reader}.csv')\n","\n","  association_data = Path(f'/content/drive/MyDrive/MoTR/local_coherence_associations/local_coherence_associations_f160/reader_{reader}_merged_denoised.csv')\n","  association_data = Path(f'/content/drive/MyDrive/MoTR/local_coherence_plots_2024/associations_reader_{reader}.csv')\n","  saccade_data = Path(f'/content/drive/MyDrive/MoTR/local_coherence_plots_2024/Saccades_reader_{reader}.csv')\n","\n","  dfw = pd.read_csv(reading_data)\n","  dff = pd.read_csv(association_data)\n","  dft = pd.read_csv(trial_data, sep='\\t')\n","\n","  df_associations = pd.read_csv(association_data)\n","  df_saccades = pd.read_csv(saccade_data)\n","  # Group association and saccade data by 'para_nr'\n","  grouped_association_data = df_associations.groupby('para_nr')\n","  grouped_saccade_data = df_saccades.groupby('para_nr')\n","\n","  # Create a PDF file to save the plots\n","  pdf_filename = f'/content/drive/MyDrive/MoTR/local_coherence_plots_2024/reader_{reader}_plots2.pdf'\n","  pdf_pages = pdf.PdfPages(pdf_filename)\n","\n","\n","\n","  for condition in [1, 2, 3, 4, 5, 6]:\n","    dfw_cond = dfw[dfw['cond_id'] == condition]\n","    dft_cond = dft[dft['condition_id'] == condition]\n","    dff_cond = dff[dff['cond_id'] == condition]\n","\n","    dff_cond = dff_cond[['para_nr', 'word_nr', 'word', 'duration', 'start_t', 'end_t', 'x_mean', 'y_mean']]\n","\n","    dft_cond = dft_cond[['experiment_id', 'condition_id', 'item_id', 'text', 'response_true']]\n","    new_column_name = {'experiment_id': 'expr_id', 'condition_id': 'cond_id', 'item_id': 'para_nr'}\n","    dft_cond = dft_cond.rename(columns=new_column_name)\n","\n","    df_cond = pd.merge(dfw_cond, dft_cond, on=['expr_id', 'cond_id', 'para_nr'])\n","    df_cond = df_cond.assign(correct=0)\n","    df_cond.loc[df_cond['response_true'] == df_cond['response'], 'correct'] = 1\n","    df_cond = df_cond[['cond_id', 'para_nr', 'word_nr', 'word', 'text', 't', 'x', 'y', 'response', 'response_true', 'correct']]\n","    grouped_df_cond = df_cond.groupby('para_nr')\n","    grouped_dff_cond = dff_cond.groupby('para_nr')\n","\n","    num_rows = 4\n","    num_cols = 2\n","    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 16))\n","    plt.subplots_adjust(hspace=0.3)\n","    visualization_multi(grouped_df_cond, grouped_dff_cond, grouped_association_data, grouped_saccade_data, fig, axes)\n","\n","    # Save the current plot to the PDF file\n","    pdf_pages.savefig(fig)\n","\n","  # Close the PDF file\n","  pdf_pages.close()\n","\n","  print(f'Plots saved to {pdf_filename}.')\n"]},{"cell_type":"markdown","metadata":{"id":"uim0gxkWGByi"},"source":["# For one participant, reading one text, plot the reading path."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcue-kYCHnK6"},"outputs":[],"source":["trial_data = Path('/content/drive/MyDrive/Motr/trial_data/localCoherence.tsv')\n","\n","reading_data = Path(f'/content/drive/MyDrive/Motr/local_coherence_cleaned_raw_association_not_merged/reader_{reader}.csv')\n","\n","association_data = Path(f'/content/drive/MyDrive/Motr/local_coherence_associations_f160/reader_{reader}_merged_denoised.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1681754765283,"user":{"displayName":"Cuier D","userId":"13036128299667319583"},"user_tz":-120},"id":"mN8K47ciX7l9","outputId":"fc488746-29a7-4a4c-fb45-bbf14357b2dc"},"outputs":[{"data":{"text/plain":["PosixPath('/content/drive/MyDrive/Motr/local_coherence_cleaned_raw_association_not_merged/reader_121.csv')"]},"execution_count":737,"metadata":{},"output_type":"execute_result"}],"source":["reading_data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":345,"status":"ok","timestamp":1681754765626,"user":{"displayName":"Cuier D","userId":"13036128299667319583"},"user_tz":-120},"id":"2ToPNOHvYMqA","outputId":"fde44562-91bc-4d03-9312-0fa16668b768"},"outputs":[{"data":{"text/plain":["PosixPath('/content/drive/MyDrive/Motr/local_coherence_associations_f160/reader_121_merged_denoised.csv')"]},"execution_count":738,"metadata":{},"output_type":"execute_result"}],"source":["association_data"]},{"cell_type":"markdown","metadata":{"id":"t77aqcp8jlfO"},"source":["### Read in whole data and association data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b3SpFozkjkQk"},"outputs":[],"source":["dfw = pd.read_csv(reading_data)\n","dff = pd.read_csv(association_data)\n","dft = pd.read_csv(trial_data, sep='\\t')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1681754766591,"user":{"displayName":"Cuier D","userId":"13036128299667319583"},"user_tz":-120},"id":"ByxRjKb-fYLR","outputId":"2e4460ef-a88f-4b5d-aac6-02ce02f34f85"},"outputs":[{"data":{"text/plain":["array([1, 2, 3, 4, 5, 6, 7, 8])"]},"execution_count":740,"metadata":{},"output_type":"execute_result"}],"source":["dfw['cond_id'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1dFFyk_SvrXa_PgXvvTuYmJNV2ptuIykY"},"executionInfo":{"elapsed":22885,"status":"ok","timestamp":1681754789472,"user":{"displayName":"Cuier D","userId":"13036128299667319583"},"user_tz":-120},"id":"We3G2_GvhRRW","outputId":"d0a88922-768f-4054-cf54-5fd511aa28d7"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import matplotlib.backends.backend_pdf as pdf\n","\n","# Create a PDF file to save the plots\n","pdf_filename = f'/content/drive/MyDrive/Motr/local_coherence_plots/f160/reader_{reader}_plots.pdf'\n","pdf_pages = pdf.PdfPages(pdf_filename)\n","\n","for condition in [1, 2, 3, 4, 5, 6]:\n","  dfw_cond = dfw[dfw['cond_id'] == condition]\n","  dft_cond = dft[dft['condition_id'] == condition]\n","  dff_cond = dff[dff['cond_id'] == condition]\n","\n","  dff_cond = dff_cond[['para_nr', 'word_nr', 'word', 'duration', 'start_t', 'end_t', 'x_mean', 'y_mean']]\n","\n","  dft_cond = dft_cond[['experiment_id', 'condition_id', 'item_id', 'text', 'response_true']]\n","  new_column_name = {'experiment_id': 'expr_id', 'condition_id': 'cond_id', 'item_id': 'para_nr'}\n","  dft_cond = dft_cond.rename(columns=new_column_name)\n","\n","  df_cond = pd.merge(dfw_cond, dft_cond, on=['expr_id', 'cond_id', 'para_nr'])\n","  df_cond = df_cond.assign(correct=0)\n","  df_cond.loc[df_cond['response_true'] == df_cond['response'], 'correct'] = 1\n","  df_cond = df_cond[['cond_id', 'para_nr', 'word_nr', 'word', 'text', 't', 'x', 'y', 'response', 'response_true', 'correct']]\n","  grouped_df_cond = df_cond.groupby('para_nr')\n","  grouped_dff_cond = dff_cond.groupby('para_nr')\n","\n","  num_rows = 4\n","  num_cols = 2\n","  fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 16))\n","  plt.subplots_adjust(hspace=0.3)\n","  visualization_multi(grouped_df_cond, grouped_dff_cond, fig, axes)\n","\n","  # Save the current plot to the PDF file\n","  pdf_pages.savefig(fig)\n","\n","# Close the PDF file\n","pdf_pages.close()\n","\n","print(f'Plots saved to {pdf_filename}.')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnVddIte2nIh"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GkBK_71Wi-6V"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"huxlcP4Pf5K8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qmLK8nLf5NF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dSZ_c1hpf5Pd"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMS6UD6ZmHLzOuHDFnLmnxP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}